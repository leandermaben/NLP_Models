{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis With Gradient Boost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/leander/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.sentiment.util import mark_negation\n",
    "from nltk.tokenize.casual import TweetTokenizer\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>ids</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>TweetText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811372</td>\n",
       "      <td>Mon Apr 06 22:20:00 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>joy_wolf</td>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target         ids                          date      flag           user  \\\n",
       "0       0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY  scotthamilton   \n",
       "1       0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY       mattycus   \n",
       "2       0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY        ElleCTF   \n",
       "3       0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY         Karoli   \n",
       "4       0  1467811372  Mon Apr 06 22:20:00 PDT 2009  NO_QUERY       joy_wolf   \n",
       "\n",
       "                                           TweetText  \n",
       "0  is upset that he can't update his Facebook by ...  \n",
       "1  @Kenichan I dived many times for the ball. Man...  \n",
       "2    my whole body feels itchy and like its on fire   \n",
       "3  @nationwideclass no, it's not behaving at all....  \n",
       "4                      @Kwesidei not the whole crew   "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('./assets/datasets/sentiment.csv',encoding='latin-1')\n",
    "DATASET_COLUMNS = [\"target\", \"ids\", \"date\", \"flag\", \"user\", \"TweetText\"]\n",
    "df.columns = DATASET_COLUMNS\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replace urls,usernames .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "vocab=[]\n",
    "punctuations=['.','*','!']\n",
    "for x in df[\"TweetText\"]:\n",
    "    x=re.sub(\"http://.* \",'URL ',x)\n",
    "    x=re.sub(\"#.* \",'HASHTAG ',x)\n",
    "    x=re.sub(\"@.* \",'USER ',x)\n",
    "    for z in punctuations:\n",
    "        x=x.replace(z,' ')\n",
    "    vocab.extend(x.split(' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To be used to classify into 3 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos=[]\n",
    "neg=[]\n",
    "neutral=[]\n",
    "for x in df[\"target\"]:\n",
    "    if x == 0:\n",
    "        neg.append(1)\n",
    "        pos.append(0)\n",
    "        neutral.append(0)\n",
    "    if x == 2:\n",
    "        neg.append(0)\n",
    "        pos.append(0)\n",
    "        neutral.append(1)\n",
    "    if x == 4:\n",
    "        neg.append(0)\n",
    "        pos.append(1)\n",
    "        neutral.append(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negation Marking and Stopword removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords=set(nltk.corpus.stopwords.words('english'))\n",
    "tokenizer=TweetTokenizer(reduce_len=True,strip_handles=True,preserve_case=False)\n",
    "X=[]\n",
    "for i in df[\"TweetText\"]:\n",
    "    X.append(tokenizer.tokenize(i))\n",
    "for i in range(len(X)):\n",
    "    X[i]=mark_negation(X[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_stop=[]\n",
    "for i in stopwords:\n",
    "    neg_stop.append(i+\"_NEG\")\n",
    "stopwords=set(nltk.corpus.stopwords.words('english'))\n",
    "stop=list(stopwords)\n",
    "stop.extend(neg_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(X)):\n",
    "    X[i]=[t for t in X[i] if t not in stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=[]\n",
    "for i in X:\n",
    "    s=\"\"\n",
    "    for j in i:\n",
    "       s+=(j+\" \")\n",
    "    corpus.append(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Vectorizing for bag of Words Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(max_features=7000)\n",
    "count_vector=cv.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(count_vector, df[\"target\"], test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_clf = XGBClassifier(silent=False,\n",
    "                      scale_pos_weight=1,\n",
    "                      learning_rate=0.01,  \n",
    "                      colsample_bytree = 0.8,\n",
    "                      subsample = 0.8,\n",
    "                      objective='binary:logistic', \n",
    "                      n_estimators=2000, \n",
    "                      reg_alpha = 0.1,\n",
    "                      max_depth=4, \n",
    "                      gamma=5,\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=0.8, gamma=5,\n",
       "              learning_rate=0.01, max_delta_step=0, max_depth=4,\n",
       "              min_child_weight=1, missing=None, n_estimators=2000, n_jobs=1,\n",
       "              nthread=None, objective='binary:logistic', random_state=0,\n",
       "              reg_alpha=0.1, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "              silent=False, subsample=0.8, verbosity=1)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_clf.fit(X_train, y_train, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7190283987854985\n"
     ]
    }
   ],
   "source": [
    "score = xgb_clf.score(X_train, y_train)\n",
    "print(score)\n",
    "score = xgb_clf.score(X_test, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=pd.read_csv('./assets/datasets/tweet_data.csv')\n",
    "\n",
    "for x in df2[\"text\"]:\n",
    "    x=re.sub(\"http://.* \",'URL ',x)\n",
    "    x=re.sub(\"#.* \",'HASHTAG ',x)\n",
    "    x=re.sub(\"@.* \",'USER ',x)\n",
    "    for z in punctuations:\n",
    "        x=x.replace(z,' ')\n",
    "    vocab.extend(x.split(' '))\n",
    "\n",
    "vocab=set(vocab)\n",
    "\n",
    "corpus=[]\n",
    "for x in df[\"TweetText\"]:\n",
    "    x=re.sub(\"http://.* \",'URL ',x)\n",
    "    x=re.sub(\"#.* \",'HASHTAG ',x)\n",
    "    x=re.sub(\"@.* \",'USER ',x)\n",
    "    for z in punctuations:\n",
    "        x=x.replace(z,' ')\n",
    "    corpus.append(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Model and Other Objects(For fututre training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "outfile = open('./saved_models/XGBoost','wb')\n",
    "pickle.dump({\n",
    "            'model': xgb_clf,\n",
    "            'count_vector': count_vector,\n",
    "            'stop_words':stop,\n",
    "            'corpus':corpus\n",
    "            },outfile)\n",
    "outfile.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
